{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338d9836",
   "metadata": {},
   "source": [
    "# CIFAR10 - lets do this\n",
    "\n",
    "Let us start with imports and a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb38327",
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "N_CLASSES = 10 # There are 10 classes in the CIFAR10 dataset\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "BATCHSIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822803a",
   "metadata": {},
   "source": [
    "### Our Model\n",
    "\n",
    "Architecture (Is this the real name for it?)  \n",
    "Convolutional Layer 1 with 50 filters and a 3 by 3 kernel and rectified linear activation  \n",
    "Convolution:  Using Filters to decide what a neuron does  \n",
    "Filters+Kernel: established image processing filters to detect corners, contrasts and the like  \n",
    "Rectified Linear Activation:  The clearer the pattern the filter finds, the stronger the neuron fires  \n",
    "A second Convolutional Layer with the same parameters  \n",
    "A pooling layer. Pooling condenses the given results down to a smaller sized data packet  \n",
    "A fully connected layer with 512 neurons (?) and rectified linear activation: All input neurons in the layer are connected to all output layers. What connections matter will be found out while training (?).  \n",
    "And a sinoid output (as far as I can tell): Ouputs are squashed to values between -1 and 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0c70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_input, n_classes=N_CLASSES, data_format='channels_last'):\n",
    "    convo1_out = teflow.layers.conv2d(model_input, \n",
    "                                    filters=50,\n",
    "                                    kernel_size=(3,3),\n",
    "                                    padding='same',\n",
    "                                    data_format=data_format,\n",
    "                                    activation=tf.nn.relu)\n",
    "    convo2_out = teflow.layers.conv2d(convo1_out,\n",
    "                                    filters=50,\n",
    "                                    kernel_size=(3,3),\n",
    "                                    padding='same',\n",
    "                                    data_format=data_format,\n",
    "                                    activation=tf.nn.relu)\n",
    "    pooling_output = teflow.layers.max_pooling2d(convo2_out,\n",
    "                                               pool_size=(2, 2),\n",
    "                                               strides=(2, 2),\n",
    "                                               padding='valid',\n",
    "                                               data_format=data_format)\n",
    "    flattened_vector = tf.reshape(pooling_output, shape=[-1, 50*16*16])\n",
    "    fully_connected = tf.layers.dense(flattened_vector, \n",
    "                                      512, \n",
    "                                      activation=tf.nn.relu)\n",
    "    output_logits = tf.layers.dense(fully_connected, n_classes, name='output')\n",
    "    return output_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c3efb",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "A loss function is needed. It calculates if our results are getting better or worse.  \n",
    "We use \"cross_entropy\" here, which calculates the amount of infromation that is needed to discern two packages of information.  \n",
    "We also need an optimization function that calculates from the loss, how much we have \n",
    "to adjust the \"weights\" of the neurons. \"Weight\" means, how much the neuron/connection matters.  \n",
    "We use something called Stochastic Gradient Descent, which is largely black magic to me.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79f66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_training(model_output_logits, labels, learning_rate=LR, momentum=MOMENTUM):\n",
    "    cross_entropy_result = teflow.nn.sparse.softmax_cross_entropy_with_logits(logits=model_output_logits,\n",
    "                                                                             labels=labels)\n",
    "    loss = teflow.reduce_mean(cross_entropy)\n",
    "    optimizer = teflow.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                              momentum = momentum)\n",
    "    return optimizer.minimizeClass(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467fd7a",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The Data we get needs to be preprocessed a bit, because it is a publicly available set not created for our case. So it needs to be made compatible to the model we will create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1260f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x_train, y_train, x_test, y_test): #those are arrays and vectors\n",
    "    # scale between 0 and 1\n",
    "    x_train = x_train / 255.0 # why the fuck does that work, if x_train is an array\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    # reshape to push the color channels into the back\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32)\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    x_train = numpy.swapaxes(x_train, 1, 3)\n",
    "    x_test = numpy.swapaxes(x_test, 1, 3)\n",
    "    \n",
    "    return(x_train.astype(numpy.float32),\n",
    "          y_train.astype(numpy.int32),\n",
    "          x_test.astype(numpy.floag32),\n",
    "          y_test.astype(numpy.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab64ecf",
   "metadata": {},
   "source": [
    "### Batch Data Generator\n",
    "\n",
    "This will continuously spit out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "444b3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_from(Data, labels, batchsize=BATCHSIZE, shuffle=False):\n",
    "    if(len(Data) != len(labels)):\n",
    "        raise Exception(\"The length of the data does not match the length of the label set \".format(len(Data), len(labels)))\n",
    "    \n",
    "    if shuffle:\n",
    "        Data, labels = shuffle_data(Data, labels)\n",
    "        \n",
    "    for i in range(0, len(Data), batchsize):\n",
    "        yield Data[i:i + batchsize], labels[i:i + batchsize]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
